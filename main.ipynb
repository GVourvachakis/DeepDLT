{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/GVourvachakis/DeepDLT/blob/main/main.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" width=\"300\" height=\"auto\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Components\n",
    "\n",
    "### create_dataset.py \n",
    "Creates directory \"./dataset\" of **cropped and brightness-varying (by $\\pm$ 20%) 64x64 images** (create_dataset() function) in BMP (bitmap) format for storage efficiency and their respective **csv files** (create_data_with_labels_csv() function) splitted as training, validation, and testing datasets sampled from the excel file \"./images/all_images.xlsx\".\n",
    "\n",
    "### dataset_loader.py\n",
    "Construct flexible/modular **custom dataset class** LaserDataset(Dataset) with Ordinal encoded \"PP1\" categorical feature and respective train/val/test dataloaders (prepare_and_load_data() function) .\n",
    "\n",
    "### stratified_split.py\n",
    "Contains a subroutine for k-fold label-wise cross-validation splitting (k_fold_cross_validation() function, with fold=5 as default) and the main execution/development of the folds under a multil-label cross-validation() splitting scheme (main_cross_validation() function) .\n",
    "\n",
    "### training_pipeline.py \n",
    "Implements a complete training pipeline for an autoencoder that not only monitors basic loss values but also image quality metrics (PSNR and SSIM), while saving the best performing models according to each metric.\n",
    "\n",
    "### label_training.py\n",
    "Systematically evaluates how well the model performs on different types of input features using k-fold cross-validation, tracking performance metrics and providing visual feedback for each fold while maintaining separate results for each feature type.\n",
    "\n",
    "### inference.py\n",
    "Contains functions providing comprehensive visualization and analysis tools for examining how well autoencoder models are performing, both through numerical metrics and visual comparisons, while also enabling exploration of the VAE's generative capabilities (the latter one needs further development).\n",
    "\n",
    "### train_vae.py\n",
    "Trains a Variational Autoencoder (VAE) by iteratively processing data through training and validation phases, using either Adam or SGD optimizer. It saves checkpoints of the model's progress and keeps track of the best-performing version based on validation loss.\n",
    "\n",
    "### environment.yml\n",
    "contains all the dependencies and requirements.\n",
    "\n",
    "### main.ipynb\n",
    "Notebook where the whole training and inferencing pipeline is implemented \n",
    "\n",
    "### KFold_split.py\n",
    "Creates DataLoader instances for 5-fold cross-validation . [not-used]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect into the custom virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/georgios-vourvachakis/Desktop/DeepDLT/.venv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "!source .venv/bin/activate\n",
    "os.environ['VIRTUAL_ENV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import native python and torch dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np ; import matplotlib; import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import tqdm\n",
    "import torch ; import torchvision\n",
    "#import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import quantitative reconstruction evaluation metrics via scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25.0\n"
     ]
    }
   ],
   "source": [
    "import skimage as ski\n",
    "print(ski.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.3\n",
      " matplotlib:\t3.10.0\n",
      " numpy:\t\t2.2.1      \n",
      " pandas:\t2.2.3\n",
      " tqdm:\t\t4.67.1      \n",
      " torch:\t\t2.5.1+cu124\n",
      " torchvision:\t0.20.1+cu124      \n",
      " skimage:\t0.25.0\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(f\" matplotlib:\\t{matplotlib.__version__}\\n numpy:\\t\\t{np.__version__}\\\n",
    "      \\n pandas:\\t{pd.__version__}\\n tqdm:\\t\\t{tqdm.__version__}\\\n",
    "      \\n torch:\\t\\t{torch.__version__}\\n torchvision:\\t{torchvision.__version__}\\\n",
    "      \\n skimage:\\t{ski.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_loader import prepare_and_load_data #, LaserDataset\n",
    "from create_dataset import create_dir\n",
    "\n",
    "# class CNNAutoencoder is exposed in __init__.py\n",
    "\n",
    "# Directory tructure:\n",
    "# DeepDLT/\n",
    "# ├── models/\n",
    "# │   ├── __init__.py\n",
    "# │   └── autoencoder.py\n",
    "#     └── vae.py\n",
    "# └── autoencoder.ipynb.py\n",
    "from models.autoencoder import CNNAutoencoder \n",
    "\n",
    "from training_pipeline import train_model, load_checkpoint\n",
    "from inference import plotting, visualize_reconstruction\n",
    "\n",
    "# for k-fold label stratification\n",
    "from label_training import train_and_evaluate_kfold\n",
    "\n",
    "# VAE components\n",
    "from models.vae import CNNVariationalAutoencoder\n",
    "from train_vae import train_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct directory of augmented images along with train/val/test csv datasets\n",
    "(given the directory \"./datasets\" doesn't exist already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(\"./datasets\"):\n",
    "#     subprocess.run([\"python\", \"create_dataset.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate uniform label distribution-aware 5-fold cross-validation data (better *generalization*, acounting for *outliers*, and preventing *overfitting*) [given there are train/val/test files to sample from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(\"./datasets/data_with_labels_csv\"):\n",
    "#     subprocess.run([\"python\", \"stratified_split.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete preprocessing pipeline**:\n",
    "create_dataset , data_with_labels_csv and globally create train/val/test Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu, workers = 4\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "input_dirs = [\n",
    "                '2020-4-30 tuning ripple period',\n",
    "                '2020-6-9 Crossed polarized',\n",
    "                'Paper Data/Double pulses',\n",
    "                'Paper Data/Repetition 6p & 2p 29-4-2020',\n",
    "                'Paper Data/Single pulses 2p',\n",
    "                'Paper Data/Single pulses 4 and half 6',\n",
    "                'Paper Data/Repetition 6p & 2p 29-4-2020/Details'\n",
    "             ]\n",
    "    \n",
    "base_path = \"./images\"\n",
    "excel_path = \"./images/all_images.xlsx\" # sample data for train/val/test csv files\n",
    "csv_output_path = \"./datasets/data_with_labels_csv\"\n",
    "\n",
    "dim = 64 # set dimensions of augmented images\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "workers = 2 if device=='cuda' else 4  # set the workers for the dataloaders. Suggested workers = 2 when on cuda\n",
    "\n",
    "print(f\"Using {device}, workers = {workers}\")\n",
    "\n",
    "images_path = f'./datasets/2023_im_dataset_{dim}x{dim}'\n",
    "output_dir_images = create_dir(images_path)\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_and_load_data(\n",
    "                                                                input_dirs,\n",
    "                                                                base_path,\n",
    "                                                                output_dir_images,\n",
    "                                                                excel_path,\n",
    "                                                                csv_output_path,\n",
    "                                                                cropped_dim=dim,\n",
    "                                                                num_workers=workers,\n",
    "                                                                batch_size=32\n",
    "                                                             )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initialization from \"./DeepDLT/models\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = CNNAutoencoder(activation_function='relu', dropout_strength=0.3, filter_dim=5).to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = 'Adam'\n",
    "epochs = 2\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, psnr_values, ssim_values = train_model(model, train_loader, val_loader, device,\n",
    "                                                                 optimizer=optimizer, num_epochs=epochs, learning_rate=learning_rate,\n",
    "                                                                 checkpoint_name='model_checkpoint', # saving a checkpoint model every 10 epochs\n",
    "                                                                 best_metric_checkpoint_name='best_model') # saving best models for loss, psnr, and ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Checkpoint for Inference and/or Resuming Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "file_path =  f'./models_history_{optimizer}/model_checkpoint.pt'\n",
    "\n",
    "model, optimizer, start_epoch, loss = load_checkpoint(model, optimizer, file_path, lr=learning_rate)\n",
    "\n",
    "# Set model to eval mode for evaluation or train mode to continue training\n",
    "model.eval()  # For evaluation\n",
    "# Or:\n",
    "# model.train()  # For resuming training\n",
    "\n",
    "print(f\"Model restored to epoch {start_epoch} with loss {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue Training the model\n",
    "train_losses, val_losses, psnr_values, ssim_values = train_model(model, train_loader, val_loader, device,\n",
    "                                                                 optimizer='Adam', start_epoch=start_epoch, num_epochs=3, learning_rate=1e-2,\n",
    "                                                                 checkpoint_name='model_checkpoint', \n",
    "                                                                 best_metric_checkpoint_name='best_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the loss curves, PSNR and SSIM values accross epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting(train_losses=train_losses, val_losses=val_losses, psnr_values=psnr_values, ssim_values=ssim_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Reconstruction on Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction on Training Data\n",
    "print(\"Reconstruction on Training Data:\")\n",
    "visualize_reconstruction(train_loader, model, model_label='Autoencoder', device=device, num_images=5)\n",
    "\n",
    "# Reconstruction on Test Data\n",
    "print(\"Reconstruction on Test Data:\")\n",
    "visualize_reconstruction(test_loader, model, model_label='Autoencoder', device=device, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label distribution-aware k-fold cross-validation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Generate 5-fold splits across labels\n",
    "if os.path.exists(\"./datasets/data_with_labels_csv\"):\n",
    "    subprocess.run([\"python\", \"stratified_split.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "#Output:\n",
    "\n",
    "# Fold-wise train, validation, and test losses for each feature.\n",
    "# Average test loss per feature.\n",
    "# Printed comparison of reconstruction difficulty across features.\n",
    "\n",
    "# Stratified splits path\n",
    "stratified_dir = \"./datasets/label_aware_splitting_data\"\n",
    "num_folds = 2 # for demonstration purposes\n",
    "\n",
    "labels = ['angle', 'EP1', 'NP', 'PP1']\n",
    "\n",
    "# Train and evaluate the model across all folds\n",
    "# Parameters:\n",
    "# model_class, fold_dir, num_folds, device, features, optimizer = \"Adam\",\\\n",
    "# num_workers=4, criterion=nn.MSELoss(), num_epochs=10, learning_rate=1e-3\n",
    "train_and_evaluate_kfold(CNNAutoencoder, stratified_dir, num_folds, device, features=labels, num_workers=workers, num_epochs=1, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# VAE Parameters:\n",
    "# activation_function: str = 'relu', dropout_strength: float = 0.2, latent_dim: int = 128\n",
    "        \n",
    "vae_model = CNNVariationalAutoencoder(activation_function='leakyrelu',dropout_strength=0.2).to(device)\n",
    "learning_rate = 1e-4\n",
    "optimizer = 'Adam'\n",
    "epochs = 2 # for demonstration purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE's training initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Parameters:\n",
    "# model, train_loader, val_loader, device, optimizer: str = 'Adam', start_epoch=0, num_epochs=100,\\\n",
    "# learning_rate=1e-3, checkpoint_name='vae_checkpoint', best_metric_checkpoint_name='best_vae_model'\n",
    "train_losses, val_losses = train_vae(vae_model, train_loader, val_loader, device,\n",
    "                                     optimizer=optimizer, num_epochs=epochs, learning_rate=learning_rate,\n",
    "                                     checkpoint_name='model_checkpoint', # saving a checkpoint model every 10 epochs\n",
    "                                     best_metric_checkpoint_name='best_model') # saving best models for loss, psnr, and ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image reconstruction evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Reconstruction on Training Data\n",
    "print(\"Reconstruction on Training Data:\")\n",
    "visualize_reconstruction(train_loader, vae_model, model_label='Variational Autoencoder', device=device, num_images=5)\n",
    "\n",
    "# Reconstruction on Test Data\n",
    "print(\"Reconstruction on Test Data:\")\n",
    "visualize_reconstruction(test_loader, vae_model, model_label='Variational Autoencoder', device=device, num_images=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
